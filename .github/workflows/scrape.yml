name: Scrape TKD Events

on:
  schedule:
    - cron: "0 0,6,12,18 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checka ut repo
        uses: actions/checkout@v4

      - name: Sätt upp Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Skapa data-mapp
        run: mkdir -p data

      - name: Kör SimplyCompete scraper
        shell: python
        env:
          SC_COOKIE: ${{ secrets.SC_COOKIE }}
        run: |
          import json, sys, time, urllib.request, urllib.parse, urllib.error, os
          from datetime import datetime, timezone

          OUT_FILE  = "data/simplycompete.json"
          BASE      = "https://worldtkd.simplycompete.com"
          COOKIE    = os.environ.get("SC_COOKIE", "").strip()

          if not COOKIE:
              print("SC_COOKIE saknas", file=sys.stderr)
              out = {"updated_at": datetime.now(timezone.utc).isoformat(), "count": 0, "events": [], "error": "no cookie"}
              json.dump(out, open(OUT_FILE,"w"), indent=2)
              sys.exit(0)

          HEADERS = {
              "Accept": "application/json, text/plain, */*",
              "Accept-Language": "sv-SE,sv;q=0.9,en-US;q=0.8",
              "Cookie": COOKIE,
              "Referer": BASE + "/events?eventType=All&invitationStatus=all&da=false&isArchived=false&pageNumber=1&itemsPerPage=12",
              "Origin": BASE,
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/145.0.0.0 Safari/537.36",
              "sec-fetch-dest": "empty",
              "sec-fetch-mode": "cors",
              "sec-fetch-site": "same-origin",
              "Dnt": "1",
              "Priority": "u=1, i",
          }

          def fetch_page(page, size=12):
              params = urllib.parse.urlencode({
                  "da": "true",
                  "eventType": "All",
                  "invitationStatus": "all",
                  "isArchived": "false",
                  "itemsPerPage": size,
                  "pageNumber": page,
              })
              url = BASE + "/events/eventList?" + params
              req = urllib.request.Request(url, headers=HEADERS)
              try:
                  with urllib.request.urlopen(req, timeout=30) as r:
                      body = r.read().decode("utf-8")
                      if not body.strip() or body.lstrip()[0] not in ("[", "{"):
                          print("Inte JSON: " + body[:150], file=sys.stderr)
                          return None
                      data = json.loads(body)
                      print("  Svar-typ: " + type(data).__name__ + ", " + (
                          "lista med " + str(len(data)) + " items" if isinstance(data, list)
                          else "nycklar: " + str(list(data.keys())[:8])
                      ))
                      if isinstance(data, list): return data
                      for k in ["events","event","items","results","eventList","list","content","records","tournaments"]:
                          if k in data and isinstance(data[k], list):
                              print("  -> hittade '" + k + "' med " + str(len(data[k])) + " items")
                              return data[k]
                      # Nested i data["data"]
                      nested = data.get("data") if isinstance(data.get("data"), dict) else None
                      if nested:
                          for k in ["event","events","items","list","results"]:
                              if k in nested and isinstance(nested[k], list):
                                  print("  -> nested data." + k + ": " + str(len(nested[k])) + " items")
                                  # Logga rådata för första eventet
                                  if nested[k]:
                                      first = nested[k][0]
                                      if isinstance(first, dict):
                                          print("  -> Första event-nycklar: " + str(list(first.keys())))
                                          # Logga nested data-fältet
                                          d = first.get("data","")
                                          if isinstance(d, str) and d:
                                              import json as _jj
                                              try:
                                                  pd = _jj.loads(d)
                                                  print("  -> data-nycklar: " + str(list(pd.keys())[:20]))
                                                  print("  -> name=" + str(pd.get("name")) + " city=" + str(pd.get("city") or pd.get("venuCity") or pd.get("venueCity")) + " country=" + str(pd.get("country") or pd.get("venueCountry")))
                                              except: print("  -> data ej JSON")
                                  return nested[k]
                      print("  -> Inget hittat. Raa svar: " + body[:300], file=sys.stderr)
                      return []
              except urllib.error.HTTPError as e:
                  print("HTTPError " + str(e.code) + " sida " + str(page), file=sys.stderr)
                  return None
              except Exception as e:
                  print("Fel: " + str(e), file=sys.stderr)
                  return None

          def norm(raw):
              # Namn finns ofta nested i raw["data"] som JSON-sträng
              nested_data = {}
              if isinstance(raw.get("data"), str):
                  try:
                      import json as _j
                      nested_data = _j.loads(raw["data"])
                  except: pass
              elif isinstance(raw.get("data"), dict):
                  nested_data = raw["data"]

              name = (
                  raw.get("name") or raw.get("eventName") or raw.get("title") or
                  nested_data.get("name") or nested_data.get("eventName") or ""
              ).strip()
              if not name: return None

              eid  = raw.get("id") or raw.get("eventId") or raw.get("slug") or ""
              url  = BASE + "/events/" + str(eid) if eid else BASE + "/events"

              date = next((raw[f] for f in ["startDate","meetingStartDate","eventDate","start_date","date","startDateTime"] if raw.get(f)), None)
              dl   = next((raw[f] for f in ["registrationDeadline","deadline","registrationClose","closeDate","regDeadline"] if raw.get(f)), None)

              city    = raw.get("venueCity") or raw.get("city") or raw.get("venue") or nested_data.get("city") or ""
              country = raw.get("venueCountry") or raw.get("country") or raw.get("countryName") or nested_data.get("country") or ""
              return {"name":name,"url":url,"date":date,"deadline":dl,"location":", ".join(filter(None,[city,country])),"country":country,"source":"World Taekwondo (SimplyCompete)"}

          all_events = []
          page = 1
          retries = 0
          while page <= 30:
              print("Sida " + str(page) + "...")
              items = fetch_page(page, size=12)
              if items is None:
                  # 403 - vänta och försök igen
                  if retries < 3:
                      retries += 1
                      wait = retries * 10
                      print("  Väntar " + str(wait) + "s innan retry " + str(retries) + "...")
                      time.sleep(wait)
                      continue
                  else:
                      print("  Max retries nådda, avslutar")
                      break
              retries = 0
              if not items: break
              all_events += [e for e in (norm(r) for r in items if isinstance(r,dict)) if e]
              if len(items) < 12: break
              page += 1
              time.sleep(3)  # 3s paus mellan sidor

          out = {"updated_at": datetime.now(timezone.utc).isoformat(), "count": len(all_events), "events": all_events}
          json.dump(out, open(OUT_FILE,"w",encoding="utf-8"), ensure_ascii=False, indent=2)
          print("Sparade " + str(len(all_events)) + " events")
        continue-on-error: true

      - name: Kör MA-RegOnline scraper
        shell: python
        run: |
          import json, re, sys, urllib.request, urllib.error
          from datetime import datetime, timezone
          from html import unescape

          BASE_URL = "https://www.ma-regonline.com"
          OUT_FILE = "data/maregonline.json"

          HEADERS = {
              "Accept": "text/html,application/xhtml+xml,*/*;q=0.8",
              "Accept-Language": "en-US,en;q=0.5",
              "Connection": "keep-alive",
              "Upgrade-Insecure-Requests": "1",
              "Sec-Fetch-Dest": "document",
              "Sec-Fetch-Mode": "navigate",
              "Sec-Fetch-Site": "none",
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/122.0.0.0 Safari/537.36",
          }

          MONTHS = {"jan":1,"feb":2,"mar":3,"apr":4,"may":5,"jun":6,"jul":7,"aug":8,"sep":9,"oct":10,"nov":11,"dec":12}

          def parse_date(s):
              m = re.match(r"(\d{1,2})\s+([A-Za-z]+)\s+(\d{4})", s.strip())
              if not m: return None
              day, mon, yr = int(m.group(1)), m.group(2).lower()[:3], int(m.group(3))
              if mon not in MONTHS: return None
              try: return datetime(yr, MONTHS[mon], day).strftime("%Y-%m-%d")
              except: return None

          try:
              req = urllib.request.Request(BASE_URL + "/", headers=HEADERS)
              with urllib.request.urlopen(req, timeout=30) as r:
                  body = r.read().decode("utf-8", errors="replace")
          except Exception as e:
              print("Fel: " + str(e), file=sys.stderr)
              body = ""

          if not body or "Imunify360" in body:
              out = {"updated_at": datetime.now(timezone.utc).isoformat(), "count": 0, "events": [], "error": "blocked"}
              json.dump(out, open(OUT_FILE,"w"), indent=2)
              sys.exit(0)

          events = []
          pat = re.compile(r'<a[\s\S]*?href=["\'](/tournaments/\d+/[^"\']+)["\'][\s\S]*?>([^<]+)</a>', re.I)
          for m in pat.finditer(body):
              href = m.group(1)
              name = unescape(m.group(2)).strip()
              if not name or len(name) < 3: continue
              after = unescape(re.sub(r"<[^>]+>","", body[m.end():m.end()+300]))
              dm = re.search(r"(\d{1,2}\s+[A-Za-z]+\s+\d{4})", after)
              if not dm: continue
              d = parse_date(dm.group(1))
              if not d: continue
              try:
                  if datetime.strptime(d,"%Y-%m-%d") < datetime.now(): continue
              except: pass
              dlm = re.search(r"Deadline:\s*(\d{1,2}\s+[A-Za-z]+\s+\d{4})", after, re.I)
              dl = parse_date(dlm.group(1)) if dlm else None
              before = body[max(0,m.start()-300):m.start()]
              fm = re.search(r'<img[\s\S]*?alt=["\']([^"\']+)["\'][\s\S]*?flags[\s\S]*?>', before, re.I)
              country = unescape(fm.group(1)).strip() if fm else ""
              events.append({"name":name,"url":BASE_URL+href,"date":d,"deadline":dl,"location":country,"country":country,"source":"MA-RegOnline"})

          out = {"updated_at": datetime.now(timezone.utc).isoformat(), "count": len(events), "events": events}
          json.dump(out, open(OUT_FILE,"w",encoding="utf-8"), ensure_ascii=False, indent=2)
          print("Sparade " + str(len(events)) + " events")
        continue-on-error: true

      - name: Visa resultat
        run: |
          echo "=== SimplyCompete ==="
          python3 -c "import json; d=json.load(open('data/simplycompete.json')); print('Events:', d['count'])" 2>/dev/null || echo "Fil saknas"
          echo "=== MA-RegOnline ==="
          python3 -c "import json; d=json.load(open('data/maregonline.json')); print('Events:', d['count'])" 2>/dev/null || echo "Fil saknas"

      - name: Committa och pusha data
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/
          if git diff --staged --quiet; then
            echo "Ingen ändring"
          else
            git commit -m "chore: uppdatera events-data $(date -u +'%Y-%m-%d %H:%M UTC')"
            git push
          fi
