name: Scrape TKD Events

on:
  schedule:
    - cron: "0 0,6,12,18 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checka ut repo
        uses: actions/checkout@v4

      - name: Sätt upp Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Installera Playwright
        run: |
          pip install playwright
          playwright install chromium

      - name: Skapa data-mapp
        run: mkdir -p data

      - name: Kör SimplyCompete scraper (Playwright)
        shell: python
        run: |
          import json, sys, re
          from datetime import datetime, timezone
          from playwright.sync_api import sync_playwright

          OUT_FILE = "data/simplycompete.json"
          BASE     = "https://worldtkd.simplycompete.com"

          events = []
          api_data = []

          with sync_playwright() as p:
              browser = p.chromium.launch(headless=True)
              context = browser.new_context(
                  user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
                  viewport={"width": 1280, "height": 800},
              )

              # Fånga API-anrop som sidan gör
              captured = []
              def handle_response(response):
                  url = response.url
                  if "event" in url.lower() and response.request.resource_type in ("xhr", "fetch"):
                      try:
                          body = response.json()
                          captured.append({"url": url, "data": body})
                          print("Fångad XHR:", url)
                      except:
                          pass

              page = context.new_page()
              page.on("response", handle_response)

              print("Öppnar events-sidan...")
              try:
                  page.goto(f"{BASE}/events", wait_until="networkidle", timeout=30000)
              except Exception as e:
                  print("Timeout/fel vid sidladdning:", e, file=sys.stderr)

              # Vänta på att events laddas
              page.wait_for_timeout(5000)

              # Logga alla fångade anrop
              print(f"Antal fångade XHR/fetch: {len(captured)}")
              for c in captured:
                  print("  URL:", c["url"])
                  data = c["data"]
                  items = data if isinstance(data, list) else data.get("events") or data.get("data") or data.get("items") or []
                  if items:
                      print(f"    -> {len(items)} items")
                      api_data.extend(items)

              if not api_data:
                  # Fallback: försök extrahera direkt från DOM
                  print("Ingen API-data, försöker DOM...", file=sys.stderr)
                  html = page.content()

                  # Sök efter event-kort/rader i HTML
                  names = page.locator("[class*='event'], [class*='Event'], [data-event], tr[class*='row']").all_text_contents()
                  print(f"DOM-element funna: {len(names)}", file=sys.stderr)

                  # Logga HTML-snippet för debug
                  snippet = html[html.find("<body"):html.find("<body")+2000] if "<body" in html else html[:2000]
                  print("HTML-snippet:", snippet[:500], file=sys.stderr)

              browser.close()

          # Normalisera API-data
          def norm(raw):
              if not isinstance(raw, dict): return None
              name = (raw.get("name") or raw.get("eventName") or raw.get("title") or "").strip()
              if not name: return None
              eid  = raw.get("id") or raw.get("eventId") or raw.get("slug") or ""
              url  = f"{BASE}/events/{eid}" if eid else f"{BASE}/events"
              date = next((raw[f] for f in ["startDate","eventDate","start_date","date","startDateTime"] if raw.get(f)), None)
              dl   = next((raw[f] for f in ["registrationDeadline","deadline","registrationClose","closeDate"] if raw.get(f)), None)
              city    = raw.get("city") or raw.get("venue") or ""
              country = raw.get("country") or raw.get("countryName") or ""
              return {"name":name,"url":url,"date":date,"deadline":dl,"location":", ".join(filter(None,[city,country])),"country":country,"source":"World Taekwondo (SimplyCompete)"}

          events = [e for e in (norm(r) for r in api_data) if e]

          out = {"updated_at": datetime.now(timezone.utc).isoformat(), "count": len(events), "events": events}
          json.dump(out, open(OUT_FILE,"w",encoding="utf-8"), ensure_ascii=False, indent=2)
          print(f"Sparade {len(events)} events")
        continue-on-error: true

      - name: Kör MA-RegOnline scraper
        shell: python
        run: |
          import json, re, sys, urllib.request, urllib.error
          from datetime import datetime, timezone
          from html import unescape

          BASE_URL = "https://www.ma-regonline.com"
          OUT_FILE = "data/maregonline.json"

          HEADERS = {
              "Accept": "text/html,application/xhtml+xml,*/*;q=0.8",
              "Accept-Language": "en-US,en;q=0.5",
              "Connection": "keep-alive",
              "Upgrade-Insecure-Requests": "1",
              "Sec-Fetch-Dest": "document",
              "Sec-Fetch-Mode": "navigate",
              "Sec-Fetch-Site": "none",
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/122.0.0.0 Safari/537.36",
          }

          MONTHS = {"jan":1,"feb":2,"mar":3,"apr":4,"may":5,"jun":6,"jul":7,"aug":8,"sep":9,"oct":10,"nov":11,"dec":12}

          def parse_date(s):
              m = re.match(r"(\d{1,2})\s+([A-Za-z]+)\s+(\d{4})", s.strip())
              if not m: return None
              day, mon, yr = int(m.group(1)), m.group(2).lower()[:3], int(m.group(3))
              if mon not in MONTHS: return None
              try: return datetime(yr, MONTHS[mon], day).strftime("%Y-%m-%d")
              except: return None

          try:
              req = urllib.request.Request(BASE_URL + "/", headers=HEADERS)
              with urllib.request.urlopen(req, timeout=30) as r:
                  body = r.read().decode("utf-8", errors="replace")
          except Exception as e:
              print("Fel: " + str(e), file=sys.stderr)
              body = ""

          if not body or "Imunify360" in body:
              out = {"updated_at": datetime.now(timezone.utc).isoformat(), "count": 0, "events": [], "error": "blocked"}
              json.dump(out, open(OUT_FILE,"w"), indent=2)
              sys.exit(0)

          events = []
          pat = re.compile(r'<a[\s\S]*?href=["\'](/tournaments/\d+/[^"\']+)["\'][\s\S]*?>([^<]+)</a>', re.I)
          for m in pat.finditer(body):
              href = m.group(1)
              name = unescape(m.group(2)).strip()
              if not name or len(name) < 3: continue
              after = unescape(re.sub(r"<[^>]+>","", body[m.end():m.end()+300]))
              dm = re.search(r"(\d{1,2}\s+[A-Za-z]+\s+\d{4})", after)
              if not dm: continue
              d = parse_date(dm.group(1))
              if not d: continue
              try:
                  if datetime.strptime(d,"%Y-%m-%d") < datetime.now(): continue
              except: pass
              dlm = re.search(r"Deadline:\s*(\d{1,2}\s+[A-Za-z]+\s+\d{4})", after, re.I)
              dl = parse_date(dlm.group(1)) if dlm else None
              before = body[max(0,m.start()-300):m.start()]
              fm = re.search(r'<img[\s\S]*?alt=["\']([^"\']+)["\'][\s\S]*?flags[\s\S]*?>', before, re.I)
              country = unescape(fm.group(1)).strip() if fm else ""
              events.append({"name":name,"url":BASE_URL+href,"date":d,"deadline":dl,"location":country,"country":country,"source":"MA-RegOnline"})

          out = {"updated_at": datetime.now(timezone.utc).isoformat(), "count": len(events), "events": events}
          json.dump(out, open(OUT_FILE,"w",encoding="utf-8"), ensure_ascii=False, indent=2)
          print("Sparade " + str(len(events)) + " events")
        continue-on-error: true

      - name: Visa resultat
        run: |
          echo "=== SimplyCompete ==="
          python3 -c "import json; d=json.load(open('data/simplycompete.json')); print('Events:', d['count'])" 2>/dev/null || echo "Fil saknas"
          echo "=== MA-RegOnline ==="
          python3 -c "import json; d=json.load(open('data/maregonline.json')); print('Events:', d['count'])" 2>/dev/null || echo "Fil saknas"

      - name: Committa och pusha data
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/
          if git diff --staged --quiet; then
            echo "Ingen ändring"
          else
            git commit -m "chore: uppdatera events-data $(date -u +'%Y-%m-%d %H:%M UTC')"
            git push
          fi
